{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01344aee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6956f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from consts import *\n",
    "from grid_funcs import *\n",
    "from ang_funcs import *\n",
    "from basic_funcs import *\n",
    "\n",
    "def process_galaxy(galaxy_file: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Takes in a csv galaxy_file containing a list of primaries and goes \n",
    "        through each, creating a gridded catalogue for all tiles the \n",
    "        galaxies lies on and determines the count and distance.\n",
    "\n",
    "    Parameters:\n",
    "        galaxy_file (str): CSV file of galaxies and their information\n",
    "            - must include ra, dec, z, and tile name\n",
    "    \n",
    "    Returns:\n",
    "        None, writes one or more outputs to CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    galaxy_df = pd.read_csv(galaxy_file)\n",
    "    number = 0\n",
    "    #count = 0\n",
    "    for _, gal in galaxy_df.iterrows():\n",
    "        tile = gal[\"tile_name\"]\n",
    "        cat_file = os.path.join(CAT_DIR, tile + \".cat\")\n",
    "        grid_file = Path(GRID_DIR) / f\"{tile}_gridded.csv\"\n",
    "        grid_center_file = Path(GRID_CENTER_DIR) / f\"{tile}_grid_centers.csv\"\n",
    "        \n",
    "        if not grid_file.exists():\n",
    "            print(f\"Gridded catalog for {tile} not found, generating files.\")\n",
    "            cat_df = read_cat(cat_file)\n",
    "            grid_tile(tile, cat_df)\n",
    "            flag_regions(tile, grid_file)\n",
    "            compute_grid_centers(tile, cat_df)\n",
    "            add_grid_counts(grid_file, grid_center_file)\n",
    "        else:\n",
    "            if not grid_center_file.exists():\n",
    "                print(f\"Grid centers for {tile} not found, generating files.\")\n",
    "                cat_df = read_cat(cat_file)\n",
    "                compute_grid_centers(tile, cat_df)\n",
    "                add_grid_counts(grid_file, grid_center_file)\n",
    "\n",
    "        compute_primary_info(grid_center_file, gal, output_dir)\n",
    "        number += 1\n",
    "        print(f\"Processed {number}/1860 galaxies\")\n",
    "        #count += 1\n",
    "\n",
    "        #if count >= 100:\n",
    "            #break\n",
    "    return\n",
    "\n",
    "def compute_primary_info(grid_file: str, gal: pd.DataFrame,\n",
    "                         output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute angular separation from a primary's ra and dec taken from \n",
    "        gal to each of the grid centers in grid_file and write results \n",
    "        to a CSV in output_dir.\n",
    "\n",
    "    Parameters:\n",
    "        grid_file (str): File of the grid information.\n",
    "        primary_file (str): File of the primary.\n",
    "        output_dir (str): Intended directory of output.\n",
    "\n",
    "    Returns:\n",
    "        None, writes output to a csv file.\n",
    "    \"\"\"\n",
    "    grid_df = pd.read_csv(grid_file)\n",
    "    results = []\n",
    "\n",
    "    sep = angular_separation(gal[\"ra\"], gal[\"dec\"],\n",
    "                             grid_df[\"ra_center\"].values,\n",
    "                             grid_df[\"dec_center\"].values)\n",
    "\n",
    "    z = gal[\"z\"]\n",
    "    ang_d = angular_diameter_distance(z) \n",
    "\n",
    "    proj_dist = projected_distance(sep, ang_d)\n",
    "\n",
    "    for i, (s, projd) in enumerate(zip(sep, proj_dist)):\n",
    "        row = {**gal.to_dict(),\n",
    "               **grid_df.iloc[i].to_dict(),\n",
    "               \"angular_separation_deg\": s,\n",
    "               \"projected_distance_mpc\": projd}\n",
    "        results.append(row)\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "\n",
    "    primary_name = gal[SDSS_ID]\n",
    "    output_file = os.path.join(output_dir, f\"{primary_name}_info.csv\")\n",
    "    out_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved separations to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b050e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "process_galaxy(Path(\"filter_matched_obj/sdss_with_matches_obj.csv\"), SDSS_PRIMARY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26693c1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def combine_primary_info(input_dir: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Combine all individual primary information CSV files in a directory\n",
    "    into one combined CSV.\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Directory containing all individual *_info.csv files.\n",
    "        output_file (str): Path to the output combined CSV.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    csv_files = sorted(Path(input_dir).glob(\"*_info.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    combined = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            combined.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {file.name}: {e}\")\n",
    "\n",
    "    if not combined:\n",
    "        print(\"No valid dataframes to combine.\")\n",
    "        return\n",
    "\n",
    "    combined_df = pd.concat(combined, ignore_index=True)\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined {len(csv_files)} files into {output_file}\")\n",
    "\n",
    "combine_primary_info(SDSS_PRIMARY_DIR, \"all_sdss_primaries_with_mask_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
